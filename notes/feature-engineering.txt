特征工程：主要包括：Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）
http://www.csuldw.com/2015/10/24/2015-10-24%20feature%20engineering/
http://blog.csdn.net/u013547284/article/details/78460937
http://blog.csdn.net/google19890102/article/details/40019271

作者：严林
链接：https://www.zhihu.com/question/28641663/answer/41653367
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

通常而言，特征选择是指选择获得相应模型和算法最好性能的特征集，工程上常用的方法有以下：
1. 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，
但是计算相对复杂一些，得到相关性之后就可以排序选择特征了；
2. 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征，另外，记得JMLR'03上有一篇论文介绍了一种基于决策树的特征选择方法，本质上是等价的。
当选择到了目标特征之后，再用来训练最终的模型；
3. 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，
原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验；
4. 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
5. 通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，
这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。
6. 通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，
原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。
从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了


数据的标准化和归一化：
数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，
将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。其中最典型的就是数据的归一化处理，即将数据统一映射到[0,1]区间上。
目前数据标准化方法有多种，归结起来可以分为直线型方法(如极值法、标准差法)、折线型方法(如三折线法)、曲线型方法(如半正态性分布)。
不同的标准化方法，对系统的评价结果会产生不同的影响，然而不幸的是，在数据标准化方法的选择上，还没有通用的法则可以遵循
归一化的好处：
1.提升模型的收敛速度
2.提升模型的精度
3.深度学习中数据归一化可以防止模型梯度爆炸
从经验上说，归一化是让不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。

常见的归一化方法：
http://blog.csdn.net/pipisorry/article/details/52247379
1. min-max标准化/0-1标准化
2. log函数转换 new_x = log(x)/log(max_x), 样本x需大于1
3. atan函数转换 new_x = atan(x)*2/pi， 大于等于0的数据映射到[0,1]，小于0的映射到[-1,0]
4. z-score标准化（标准差标准化/零均值标准化） new_x = (x- x的均值)/x的标准差，适用于属性最大最小值未知或者有超出取值范围的离群数据的情况
5. Decimal scaling小数定标标准化 通过移动小数点位置来进行标准化，例如 -986到917的样本可标准化成 -0.986~0.917范围
6. logistic/softmax变换
7. 模糊量化模式

正则化：提高模型的泛化能力，降低过拟合
常用的正则化方法：
L1正则化，L2正则化，dropout，数据扩充
L0,L1(LASSO),L2(Ridge,岭回归)正则化浅析：
L0正则化的值是模型参数中非零参数的个数。L0正则化很难求解，是个NP难问题，因此一般采用L1正则化。
L1正则化表示各个参数绝对值之和。
L1正则化是L0正则化的最优凸近似，比L0容易求解，并且也可以实现稀疏的效果
L2正则化标识各个参数的平方的和的开方值。

协方差的意义：http://blog.csdn.net/wuhzossibility/article/details/8087863


kaggle 案例机器学习过程：
https://www.kaggle.com/startupsci/titanic-data-science-solutions
一。拿到数据后的基本分析：
1. 分析样本中哪些属性是文本的，哪些是数字的（连续 or 离散，普通数字 or 时序）
2. 哪些属性值有错误值或书写错误
3. 哪些熟悉有空值（blank or null or empty）
4. 各属性的数据类型
5. 数值类型的属性分布及其与lable的关系
6. 文本类型的属性分别（重复率等）和lable的关系
二。基于步骤一的结果提出一些思考：
1. 需要知道各属性和lable的相关性
2. 如果某属性与lable有相关性，需要补全属性空值
3. 校正属性（丢弃以下属性列：缺失值多，和lable没有相关性，对分类没有意义的属性）
4. 属性构建：根据已有特征构建新的特征
5. 衡量某些特征对分类的影响，如性别为男的样本分类结果为A的有80%，那么学习器的结果肯定要高于80%
三。数据预处理
1. 丢弃/校正特征/特征值
2. 用已有特征创建新的特征
3. 文本特征值转换为数值性
四。建模、预测、调参等
