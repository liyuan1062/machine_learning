参考链接：
http://blog.csdn.net/fu_shuwu/article/details/76339818

循环神经网络（recurrent neural network，RNN）
循环神经网络的主要用途是处理和预测序列数据,
从网络结构上，循环神经网络会记忆之前的信息，并利用之前的信息影响后面结点的输出。
也就是说，循环神经网络的隐藏层之间的结点是有连接的，隐藏层的输入不仅包括输入层的输出，还包括上一时刻隐藏层的输出。

循环神经网络可以被看做是同一神经网络结构在时间序列上被复制多次的结果，这个被复制多次的结构被称之为循环体
和卷积神经网络过滤器中参数是共享的类似，在循环神经网络中，循环体网络结构中的参数在不同时刻也是共享的。

训练RNN和训练传统神经网络相似，同样要使用反向传播算法，但会有一些变化。
因为参数在网络的所有时刻是共享的，每一次的梯度输出不仅依赖于当前时刻的计算结果，也依赖于之前所有时刻的计算结果。
例如，为了计算t=4时刻的梯度，需要反向传播3步，并把前面的所有梯度加和。这被称作随时间的反向传播（BPTT）
普通的用BPTT训练的RNN对于学习长期依赖（相距很长时间的依赖）是很困难的，因为这里存在梯度消失或爆炸问题

tanh和sigmoid函数在两端的梯度值都为0，接近于平行线。当这种情况出现时，我们就认为相应的神经元饱和了。
它们的梯度为0使得前面层的梯度也为0。矩阵中存在比较小的值，多个矩阵相乘会使梯度值以指数级速度下降，最终在几步后完全消失。
比较远的时刻的梯度值为0，这些时刻的状态对学习过程没有帮助，导致你无法学习到长距离依赖。
消失梯度问题不仅出现在RNN中，同样也出现在深度前向神经网中。只是RNN通常比较深（例子中深度和句子长度一致），使得这个问题更加普遍。

很容易想到，依赖于我们的激活函数和网络参数，如果Jacobian矩阵中的值太大，会产生梯度爆炸而不是梯度消失问题。
梯度消失比梯度爆炸受到了更多的关注有两方面的原因。其一，梯度爆炸容易发现，梯度值会变成NaN，导致程序崩溃。
其二，用预定义的阈值裁剪梯度可以简单有效的解决梯度爆炸问题。梯度消失出现的时候不那么明显而且不好处理。

幸运的是，已经有一些方法解决了梯度消失问题。合适的初始化矩阵W可以减小梯度消失效应，正则化也能起作用。
更好的方法是选择ReLU而不是sigmoid和tanh作为激活函数。ReLU的导数是常数值0或1，所以不可能会引起梯度消失。
更通用的方案时采用长短时记忆（LSTM）或门限递归单元（GRU）结构。LSTM在1997年第一次提出，可能是目前在NLP上最普遍采用的模型。
GRU，2014年第一次提出，是LSTM的简化版本。
这两种RNN结构都是为了处理梯度消失问题而设计的，可以有效地学习到长距离依赖，我们会在教程的下一部分进行介绍。


长短时记忆网络（LTSM, long short term memory）结构, 是最常用的RNN类型
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
http://blog.csdn.net/haluoluo211/article/details/54847866
http://blog.csdn.net/LiuPeiP_VIPL/article/details/75000742

i,f,o被称作输入、遗忘和输出门。注意到它们的计算公式是一致的，只是用了不同的参数矩阵。
它们被称作门是因为sigmoid函数把这些向量的值挤压到了0和1之间，把它们和其他的向量逐元素相乘，就定义了你想让其他向量能“剩下”多少。
输入门定义了针对当前输入得到的隐状态能留下多少。遗忘门定义了你想留下多少之前的状态。
最后，输出门定义了你想暴露多少内部状态给外部网络（更高层和下一时刻）


